
\documentclass[11pt]{article}
\usepackage{fullpage}

%\documentclass[12pt]{article}
%\input{stat-sinica-header.tex}

\usepackage[sectionbib]{natbib}

%%% Statistica Sinica formatting
% \newcommand\statSinica[1]{#1}
 \newcommand\statSinica[1]{}
%%% arXiv formatting
 \newcommand\arxiv[1]{#1}
% \newcommand\arxiv[1]{}

% sec:intro.
% sec:alg. The \code{ibpf} algorithm for likelihood maximization
% sec:sim. Testing ibpf on a measles transmission model: simulation results
% sec:model. (subsection) The measles data and model
% sec:data. Data analysis results
% sec:discussion.

<<debug,echo=F>>=
run_level <- 1
if(is.numeric(run_level)){
  lik_run_level <- run_level
}
@

\input{header.tex}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{soul}
\usepackage{xcolor}

\usepackage{url} % not crucial - just used below for the URL

%\usepackage{fullpage}
\bibliographystyle{apalike}

<<knitr_setup,include=FALSE,cache=FALSE,purl=FALSE,child="header.Rnw">>=
@

<<doParallel,include=F,cache=F>>=
cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)

ggplot2::theme_set(ggplot2::theme_bw())
@

% \date{This manuscript was compiled on \today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\statSinica{\statSinicaSetup}

\centerline{\statSinica{\large}\arxiv{\Large}\bf An iterated block particle filter for inference on coupled dynamic}

\arxiv{\vspace{1mm}}

\centerline{\statSinica{\large}\arxiv{\Large}\bf systems with shared and unit-specific parameters}

\arxiv{\vspace{2mm}}

\centerline{Edward L. Ionides$^1$, Ning Ning$^2$ and Jesse Wheeler$^1$}

\arxiv{\vspace{1mm}}

{\small
\centerline{\small University of Michigan, Department of Statistics$^1$}
\centerline{\small Texas A\&M University, Department of Statistics$^2$}
}
 \vspace{.25cm} \fontsize{9}{11.5pt plus.8pt minus.6pt}\selectfont

\statSinica{\statSinicaSubmission}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{quotation}
\noindent {\it Abstract:}
We consider inference for a collection of partially observed stochastic interacting nonlinear dynamic processes.
Each process is identified with a label, called its unit.
Here, our primary motivation arises in biological metapopulation systems, in which a unit corresponds to a spatially distinct sub-population.
Metapopulation systems are characterized by strong dependence over time within a single unit, and relatively weak interactions between units.
These properties make block particle filters effective for simulation-based likelihood evaluation.
Iterated filtering algorithms can facilitate likelihood maximization for simulation-based filters.
Here, we introduce an iterated block particle filter that can be applied when parameters are unit-specific or shared between units.
We demonstrate the proposed algorithm by performing inference on a coupled epidemiological model describing spatiotemporal measles case report data for 20 towns.

\statSinica{\statSinicaKeywords}

\end{quotation}\par

\def\thefigure{\arabic{figure}}
\def\thetable{\arabic{table}}

\renewcommand{\theequation}{\thesection.\arabic{equation}}

\statSinica{
  \fontsize{12}{14pt plus.8pt minus .6pt}\selectfont
%%  \def\spacingset#1{\renewcommand{\baselinestretch}%
%%  {#1}\small\normalsize} \spacingset{1}
}

\date{}

\section{Introduction}
\label{sec:intro}


Statistical inference for high-dimensional partially observed nonlinear dynamic systems arises in various scientific contexts.
Massive models and data sets are considered in the geophysical sciences, carried out under the name of data assimilation \citep{evensen09book}.
Population models in ecology and epidemiology can be characterized by high levels of stochasticity, nonlinearity, measurement error, and model uncertainty, leading to challenges of a somewhat different nature to those of geophysical models.
In addition, biological population systems may have a low population count, owing to a local introduction or fade-out of one or more constituent species.
Such situations may require models with integer-valued counts, rather than continuous population approximations.
Collections of biological populations measured at different spatial locations may have spatial interactions in addition to local population dynamics; such collections are called a metapopulation.
The study of spatiotemporal disease dynamics has motivated research into inference for metapopulation systems \citep{xia04,li20,park20,ionides21,cauchemez08,bjornstad19}.

Until recently, statistical inference for partially observed nonlinear biological systems was an open methodological challenge, even in the time series case \citep{bjornstad01}.
Advances in Monte Carlo methods based on particle filters have made inference accessible in many low-dimensional problems \citep{doucet11,kantas15,king16}.
However, the curse of dimensionality \citep{bengtsson08} prevents application of the basic particle filter on metapopulations with more than a few units.
Methods based on improving the proposal distribution for the particle filter may not fully resolve the curse of dimensionality \citep{snyder15}.
Previous Monte Carlo methods which can provably beat the curse of dimensionality may have limited applicability \citep{beskos17,park20,ionides21}.
Consequently, state-of-the-art scientific analysis for metapopulation dynamics depends on ensemble Kalman filter (EnKF) methods \citep{li20}.
EnKF algorithms scale well, but are founded on an approximation that can be unsuitable for discrete populations with fade-out and re-introduction dynamics and other highly non-Gaussian features \citep{ionides21}.

In Section~\ref{sec:alg}, we propose an algorithm for inference on metapopulation dynamics, which we call an iterated block particle filter (IBPF).
IBPF algorithms combine an iterated filtering likelihood maximization technique \citep{ionides15} with a block particle filter (BPF) \citep{rebeschini15}.
Iterated filtering algorithms use parameter perturbations to coerce a filtering algorithm into exploring the parameter space.
BPF algorithms address the curse of dimensionality by modifying the resampling step of a particle filter to resample independently on blocks that form a partition of the collection of units.
A previous IBPF algorithm was developed by \citet{ning21-ibpf} for the particular case in which all estimated parameters are unit-specific, that is, the dynamics and measurement process for a unit $u$ are determined by a vector of parameters $\psi_u$ specific to unit $u$.
In Section~\ref{sec:alg}, we provide a formal meaning of this assertion, together with the pseudocode for our algorithm.
We propose an extension of the aforementioned IBPF which additionally allows us to estimate a vector of shared parameters, $\phi$.
In this case, the full parameter vector is $\theta=(\phi,\psi_{1:U})$, where the $U$ units are named $\{1,\dots,\Unit\}$, which we denote by $\seq{1}{\Unit}$.
\citet{ning21-ibpf} develop a theoretical justification for their algorithm; however, our new IBPF currently relies on empirical support only.

There may be scientific interest in which parameters in a metapopulation system are best understood as being unit-specific, and which can reasonably be modeled as being shared between units.
Another relevant possibility is that a parameter may differ between units as a shared function of unit-specific covariates.
Formally, this is a special case of a shared parameter.
Addressing these issues is also a prerequisite for studying questions about the coupling of metapopulation systems using model-based inference from spatiotemporal data.
In Section~\ref{sec:data}, we will show empirically that our {\ibpf} algorithm is applies to an inference challenge in epidemiological metapopulation dynamics.
Our demonstration focuses on a data set of weekly measles incidence in 20 towns in the United Kingdom (UK) during the pre-vaccination era \citep{he10}, modeled using a previously studied metapopulation model \citep{park20,ionides21}.
Measles case reports are a longstanding benchmark problem for inference on biological dynamics, motivating the development of time series methodologies and, more recently, the progression from single populations to metapopulation systems.
Unlike previous attempts on sequential Monte Carlo inference for metapopulation models, we show that our algorithm can provide practical plug-and-play, likelihood-based inference when the parameters are either shared between units or differ between units.
Demonstrating a solution to this open problem provides numerical evidence substituting for numerical comparisons with alternative methods.

Our data analysis results do not fully resolve open questions about what models for coupling between towns are supported by the data, and which parameters should be modeled as unit-specific.
Rather, we demonstrate steps toward this research goal.
We use a simulation study, discussed in Section~\ref{sec:sim}, to show that our methodology can deliver a good approximation to the maximum likelihood estimate (MLE) when fitting the model used to simulate the data.
This allows us to interpret our data analysis results as evidence of model misspecification, providing a guide for future investigations of these data, as well as tools to carry out such investigations.

Optimization of high-dimensional, non-convex and potentially multi-modal functions, evaluated using stochastic methods, is not straightforward, even when it is possible to evaluate the function within an acceptable level of error.
Therefore, we discuss approaches that assist noisy likelihood searches, and suggest diagnostic plots to assess their success.

\section{An {\ibpf} algorithm for likelihood maximization}
\label{sec:alg}

A latent Markov process is denoted by $\{\myvec{X}_{\time},\time\in \seq{0}{\Time}\}$, with $\myvec{X}_{\time}=X_{1:\Unit,\time}$ taking values in a product space $\Xspace^\Unit$.
We define set-valued subscripts by $X_A=\{X_a,a\in A\}$ and $X_{A,B}=\{X_{a,b}, a\in A, b\in B\}$.
The discrete time process $\myvec{X}_{0:\Time}$ may arise from a continuous-time Markov process $\{\myvec{X}(t), t_0\le t\le t_{\Time} \}$ observed at times $t_{1:\Time}$, in which case, we set $\myvec{X}_\time=\myvec{X}(t_\time)$.
The initial value $\myvec{X}_{0}$ may be stochastic or deterministic.
Observations are made on each unit, modeled by an observable process $\myvec{Y}_{1:\Time}=Y_{1:\Unit,1:\Time}$ that takes values  at each time $\time$ in a product space $\Yspace^\Unit$.
Observations are modeled as conditionally independent, given the latent process.
The conditional independence of measurements applies over both time and the unit structure; thus, the collection $\big\{Y_{\unit\comma\time},\unit\in\seq{1}{\Unit},\time\in\seq{1}{\Time}\big\}$ is conditionally independent, given
$\big\{X_{\unit\comma\time},\unit\in\seq{1}{\Unit},\time\in\seq{1}{\Time}\big\}$.
We suppose the existence of a joint density $f_{\myvec{X}_{0:\Time},\myvec{Y}_{1:\Time}}$  for $X_{1:\Unit,0:\Time}$ and $Y_{1:\Unit,1:\Time}$ with respect to some appropriate measure, following a notational convention that the subscripts of $f$ denote the joint or conditional density under consideration.
We suppose that $f$ depends on a real-valued parameter vector $\theta=(\phi,\psi_{1:U})$, which we write as $\theta=\theta_{1:D}$ when we wish to concatenate the shared and unit-specific parameters into a single vector of length $D$.
The data are $\data{y}_{\unit\comma\time}$, for unit $\unit$ at time $\time$.
This model is a special case of a  partially observed Markov process (POMP), also known as a state-space model or hidden Markov model.
The additional unit structure, not generally required for a POMP, is appropriate for modeling interactions between units characterized by a spatial location; thus, we call the model a SpatPOMP.
For metapopulation models, the units are not, in general, arranged on a spatial grid, but instead comprise a collection of spatially distributed population centers.

A numerical challenge of fundamental statistical relevance is maximizing the log-likelihood function of the data, given the model, $\loglik(\theta)=\log f_{\myvec{Y}_{1:\Time}}(\data{\myvec{y}}_{1:\Time}\giventh \theta)$.
Numerical evaluation of the likelihood function is closely related to the filtering problem of evaluating $f_{\myvec{X}_n|\myvec{Y}_{1:n}}(\myvec{x}_n\given \data{\myvec{y}}_{1:n}\giventh \theta)$.
If the dynamic model is extended to include the parameters as latent variables, the filtering problem leads to the Bayesian posterior distribution, though regularization is required to make the calculation numerically tractable using Monte Carlo methods \citep{kitagawa98,janeliu01}.
Iterating this Bayesian calculation recursively targets an MLE, a strategy known as data cloning \citep{lele07,lele10}.
Adding noise to perturb the parameters in the extended model at each time point can stabilize the numerics, while still being able to approximate the MLE \citep{ionides06-pnas,ionides11,ionides15}.
Many variations on this idea have been developed using different filter methods \citep{park20,li20,ionides21,manoli15} or employing different perturbation systems \citep{doucet13,nguyen17}.

A direct approach to iterating a BPF for parameter estimation is to resample the extended model independently on each block, yielding separate collections of parameters for each block.
\citet{ning21-ibpf} prove that this IBPF algorithm targets the MLE for the special case where each parameter is localized to an individual unit, that is, when all parameters are unit-specific.
Formally, we say that a parameter for a discrete-time SpatPOMP is specific to a unit $u$ if it is involved in specifying the measurement density $f_{Y_{u,n}|X_{u,n}}$ or the transition density $f_{X_{u,n+1}|\myvec{X}_n}$, for some $n$, and it is not involved in $f_{Y_{v,m}|X_{v,m}}$ or $f_{X_{v,m+1}|\myvec{X}_m}$, for any $v\neq u$ and any $m$.
For a continuous-time SpatPOMP, we replace the requirement on $f_{X_{u,n+1}|\myvec{X}_n}$ with an equivalent requirement on a numerical solution over a small time increment $\delta$, as in equation \eqref{eq:extension}.
A parameter that is not unit-specific is said to be shared between units.
There are intermediate possibilities, where a parameter is shared for only a subset of all units, but such a parameter is formally classified as shared.
The special case in which all parameters are unit-specific may occur, but models typically have some shared parameters that arise in transition densities and/or measurement densities for multiple units.

In our approach to iterated filtering for shared parameters, we construct an extended model, within which, the shared parameters are represented as a unit-specific parameter that happens to be constant across units.
We construct a spatiotemporally extended model by supposing that a numerical solution for the transition from time $t$ to time $t+\delta$ for each unit $u$ has the functional form
\begin{equation}
\label{eq:extension}
  X_{\unit}(t+\delta) = X_\unit(t) + Q_u
    \big(\myvec{X}(t),\myvec{\eta}_t,\phi,\psi_{\unit},t,\delta \big),
\end{equation}
where the random vector $\myvec{\eta}_t=\eta_{1:\Unit,t}$ is shared, for all $u\in \seq{1}{U}$, and does not depend on $\theta$.
If a representation \eqref{eq:extension} exists, an extended model is defined by replacing $\phi$ with $\phi_{\unit}(t)$ and $\psi_u$ with $\psi_{u}(t)$.
Equation \eqref{eq:extension} implicitly defines a continuous-time extended model by the limit as $\delta\to 0$, when that limit exists, but for simulation-based methods, a numerical solution is of more immediate concern than this limit.
Admitting a minor abuse of notation, we subsequently use the density $f$ to denote both the original model and its extension for spatiotemporally varying parameters, with the context determining which one is intended.


In some situations, the extended model may be problematic; for example, it could break conservation laws obeyed by the original system.
In other situations, the extended model may make scientific sense in its own right; for example, in biological metapopulation systems, it might be scientifically meaningful to consider a model in which there is variation over space and time in the parameters that describe the local dynamics.
Here, we focus on the hypothesis that some parameters are fixed across space and time, but the specification in \eqref{eq:extension} requires that this hypothesis be nested within a more flexible alternative.



%%%%%%% ibpf BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB %%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
\setstretch{1.35}
  \caption{
  \textbf{IBPF}. \newline
  {\bf Inputs}:
  simulator for the extended model, $f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}(\myvec{x}_{\time}\given \myvec{x}_{\time-1}\giventh\myvec{\theta})$, and initialization, $f_{\myvec{X}_0}(\myvec{x}_0\giventh\myvec{\theta})$;
  evaluator for $f_{{Y}_{\unit,\time}|{X}_{\unit,\time}}({y}_{\unit,\time}\given {x}_{\unit,\time}\giventh\theta)$;
  data, $\data{\myvec{y}}_{1:\Time}$;
  number of particles, $\Np$;
  blocks, $\blocklist_{1:\Block}$;
  initial parameter swarm with decomposition into shared and unit-specific parameters, $\Theta^{0,\np}_{\unit}=(\Phi^{0,\np}_{\unit},\Psi^{0,\np}_{\unit})$;
  random walk perturbation, $\sigma_{d,\time}$; cooling rate, $a$; number of iterations, $M$; spatial autoregression, $\spatReg$.
}\label{alg:ibpfilter}
%\BlankLine
\For{$\nit\ \mathrm{in}\ \seq{1}{\Nit}$}{
  Perturb parameters:
    $\Theta^{F,\nit,\np}_{\unit,0}\sim \normal
    \left(
      \Theta^{\nit-1,\np}_{\unit}\param \sigma^2_{0} a^{2\nit/50}
    \right)$
  \;
  Initialization:  simulate $\myvec{X}_{0}^{F,\np}\sim {f}_{\myvec{X}_{0}}
    \left(\mydot\giventh{\Theta^{F,\nit,\np}_{1:\Unit,0}}\right)$\;
    \For{$\time\ \mathrm{in}\ \seq{1}{\Time}$}{ % For each time step
      Perturb parameters:
        $\Theta^{P,\nit,\np}_{\unit,\time}\sim \normal\left(
       \Theta^{F,\nit,\np}_{\unit,\time-1},\sigma^2_\time a^{2\nit/50} \right)$
     \;
      Prediction simulation: $\myvec{X}_{\time}^{P,\np}
        \sim {f}_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}\big(
        \mydot|\myvec{X}_{\time-1}^{F,\np};{\Theta^{P,\nit,\np}_{1:\Unit,\time}}\big)$
      \;
    \For{$\block\ \mathrm{in}\ \seq{1}{\Block}$}{
      Block prediction weights:
            $\displaystyle \blockweight^P_{\time,\np, \block}=
              \prod_{\unit \in \blocklist_{\block}}
              f_{Y_{\unit,\time}|X_{\unit,\time}}
              \big(
              \data{y}_{\unit,\time}\given X^{P,\np}_{\unit,\time}
                \giventh \Theta^{P,\nit,\np}_{\unit,\time} \big)$
            \nllabel{alg:bpfilter:blockweights}
      \;
      Normalize weights:
        $\tilde{\blockweight}_{\time,\np, \block}= \blockweight^P_{\time,\np,\block}\Big/
        \sum_{i=1}^{\Np}\blockweight^P_{\time,i,\block}$\;
      Select resample indices:
        $\resampleIndex_{1:\Np,\block}$ with
        $\prob\left[\resampleIndex_{\np,\block}=\altNp\right] =
        \tilde{\blockweight}_{\time,\np,\block}$
      \;
         $X_{\blocklist_{\block},\time}^{F,\np}=X_{\blocklist_{\block},\time}^{P,\resampleIndex_{\np,\block}}$,
	$\Theta_{\blocklist_{\block},\time}^{F,\np}
	  =\Theta_{\blocklist_{\block},\time}^{P,\resampleIndex_{\np,\block}}
	  =\big(\Phi_{\blocklist_{\block},\time}^{P,\resampleIndex_{\np,\block}},
	    \Psi_{\blocklist_{\block},\time}^{P,\resampleIndex_{\np,\block}}\big)$
         \nllabel{alg:bpfilter:resample}
      \;
      block mean of shared parameters: $\mu_{\block,\time}^{} = \Np^{-1}\sum_{\np=1}^{\Np} \Phi^{F,\np}_{\blocklist_{\block},\time}$
    } %End of for loop over blocks
    overall mean of shared parameters: $\mu_{\time}^{} = \Block^{-1}\sum_{\block=1}^{\Block} \mu_{\block,\time}^{}$
    \;
    autoregressive correction:
    $\Phi^{F,\np}_{\blocklist_{\block},\time} = \Phi^{F,\np}_{\blocklist_{\block},\time} + \spatReg\big( \mu_{\time}^{} - \mu_{\block,\time}^{}  \big)$
  \;
  }%End timestep for Loop
  $\Theta_{\unit}^{\nit,\np}=\Theta^{F,\nit,\np}_{\unit,\Time}$
  \;
} % end iterated filtering loop
\KwOut{
    IBPF parameter swarm, $\Theta_{\unit}^{\Nit,\np}$
}
\end{algorithm}

The IBPF algorithm described in Algorithm~\ref{alg:ibpfilter} carries out a BPF on this extended model.
The extended parameters are given independent perturbations, but a spatial autoregressive step pulls the values of the shared parameters toward their mean over the units.
Indeed, this autoregressive step is the only difference between  Algorithm~\ref{alg:ibpfilter} and the algorithm for unit-specific parameters proposed by \citet{ning21-ibpf}.
This algorithm, in turn, is essentially the IF2 algorithm of \citet{ionides15}, with the particle filter replaced with the BPF of \citet{rebeschini15}.


Algorithm~\ref{alg:ibpfilter} assumes implicit loops over $\np\ \text{in}\ \seq{1}{\Np}$ and $\unit\ \text{in}\ \seq{1}{\Unit}$,
$\normal(\mu,\Sigma)$ denotes the normal distribution with mean $\mu$ and variance matrix $\Sigma$, and
$\sigma_\time$ is a $D\times D$ diagonal matrix with entries $\sigma_{d,\time}$.
The blocks $\blocklist_{1:\Block}$ are a partition of $\seq{1}{\Unit}$.




The pseudocode in Algorithm~\ref{alg:ibpfilter} represents our implementation of an {\ibpf} as the R function \code{ibpf}, which we have contributed to the open-source package \code{spatPomp} \citep{asfaw21github,asfaw21arxiv}.
Additionally, the source code for all results presented here is available at \url{https://github.com/ionides/ibpf_article}.
Various generalizations of this implementation are possible.
For example, iterated filtering theory does not rely on parameter perturbations following the normal distribution \citep{ionides15}, though, in practice, we transform the parameters to facilitate this convenient choice (see Section~\supSecAlg).



\subsection{Algorithmic parameters}

The model parameters are optimized on a transformed scale for which unit variation is scientifically meaningful.
In practice, this means working with positive parameters on a log scale, and with $(0,1)$ interval-valued parameters on a logistic scale.
We follow standard iterated filtering practice by using independent random walks for each parameter on this transformed scale \citep{king16}.
We find that the large number of parameters following a random walk, in the presence of unit-specific parameters, can require considerably smaller random walk standard deviations than the values around $\sigma_{d,n}=0.02$ (i.e., 2\% perturbation per time point) that have been employed for iterated filtering of time series models.
After experimentation, we used $\sigma_{d,n}=0.005$ for the initial search, and $\sigma_{d,n}=0.00125$ for subsequent refinements.
Occasionally, a parameter that can be estimated precisely from the data can benefit substantially from a smaller perturbation.
This is the case for one parameter in our measles analysis, an exponent $\alpha$, for which the scale of uncertainty is an order of magnitude smaller than that of the other parameters; therefore, we scaled $\sigma_{d,n}$ for this parameter accordingly.
In principle, $\sigma_{d,n}$ can be a function of $n$ and the parameter, $d$.
The most common reason for using this flexibility is to avoid perturbing parameters during time intervals in which we have no information about these parameters.
Following an evolutionary analogy, evolution cannot operate effectively if there is mutation, but no selection.
For an initial value parameter, that specifies only the latent process value at the initial time $t_0$, we use $\sigma_{d,n}=0$ for $n\ge 1$, and we double the value of $\sigma_{d,n}$, for $n=0$.

We use $J=4000$ particles, and set the cooling rate parameter to $a=0.5$, corresponding to a 1\% reduction in the random walk standard deviation at each iteration.
We set $M=100$ optimization iterations, chosen as an empirically assessed compromise between the effort spent on each search and the number of searches conducted.
An additional discussion of algorithmic parameters is given in Section~\supSecAlg.
The only additional algorithmic parameter over previous iterated filtering algorithms is the spatial autoregressive parameter, $\spatReg$.
The results of our numerical experiments suggest that the performance is not sensitive to the choice of $\spatReg>0$ (see the Supplementary Material, Figure~\supFigSpatReg).
Consequently, here we use $\spatReg=0.1$.

Applying an IBPF to real data forces us to address problems related to model development, model misspecification, and performance in the presence of outliers.
Before doing so, we use simulated data to demonstrate the capabilities of the methodology on a correctly specified model.


<<data-plot, echo=F, fig.height=6.5, fig.width=6.5, out.width="5.5in", fig.cap = "Weekly measles case reports for 20 UK towns.",fig.show = "hold">>=
library(spatPomp)
x <- he10 <- readRDS("sim/he10.rds")
df <- as.data.frame(x)
df[x@unit_obsnames] <- log10(df[x@unit_obsnames]+1)
unit_nm <- rlang::sym(x@unitname)
df[[unit_nm]] <- factor(df[[unit_nm]], levels = x@unit_names)
g <- ggplot2::ggplot(data = df,
  mapping = ggplot2::aes(x = !!rlang::sym(x@timename),
    y = !!rlang::sym(x@unit_obsnames))) +
  ggplot2::geom_line() +
  ggplot2::facet_wrap(unit_nm)+
  ggplot2::ylab("log10(reported cases)")+
  ggplot2::xlab("Year")+
  theme(text=element_text(family="serif"))
print(g)
@

\section{Testing an IBPF on a measles transmission model}
\label{sec:measles}

Measles transmission is a useful example of epidemiological dynamics (and therefore also ecological dynamics, for a host-pathogen ecosystem), with plentiful case report data and relatively simple biology.
Model-based analyses of measles time series data have led to a better understanding of the seasonality of infectious diseases \citep{fine82}, critical community size \citep{bartlett57}, the recognition that relatively simple mechanistic models can provide a remarkably good description of the dynamics \citep{earn00}, and other foundational research on disease dynamics.
Some progress has been made on building and fitting spatiotemporal models for measles; see, for example, \citet{xia04}, \citet{eggo11}, \citet{bjornstad19}, and \citet{becker20}.
However, the lack of suitable methodology to fit and assess a flexible class of coupled models is an obstacle to further progress \citep{becker20}.
Previous methodological research has used spatiotemporal measles models as a test problem \citep{park20,ionides21}.
However, these methods have fallen short as tools for data analysis, owing to numerical considerations.
Bearing all this in mind, measles provides a natural testing ground for our new methodologies.
We demonstrate that we now have the tools to carry out likelihood maximization (and therefore, in principle, profile likelihood confidence interval construction and likelihood-based model selection) on mechanistic statistical models that are appropriate for spatiotemporal metapopulation disease data.



\subsection{The measles data, a model, and three submodels}
\label{sec:model}

\setcounter{equation}{0}

We set ourselves the task of fitting a spatiotemporal model to the case reports for 20 towns studied by \citet{he10}.
We need to be able to handle discrete case counts, which vary from zero to thousands of cases per week.
We consider a model with the same structure as that of \citet{he10}, namely, a Markov chain with gamma noise on the infection rate, but with an additional term for the transmission between cities.
This requirement limits us to plug-and-play methodologies, which are those that require simulation from the latent process model, but not the ability to  evaluate transition densities \citep{breto09,he10}.

Some previous analyses have used counts aggregated over two-week intervals \citep{park20,ionides21}, because these were available for more cities, but our goal here is to extend the analysis of \citet{he10}.
Apart from this, our model matches that of \citet{ionides21} and, for completeness, we repeat the description here.
We compartmentalize the population of each town into susceptible ($S$), exposed ($E$), infectious ($I$), and recovered/removed ($R$) categories.
The numbers of individuals in each compartment for town $\unit$ at time $t$ are denoted by integer-valued random variables $S_\unit(t)$, $E_\unit(t)$, $I_\unit(t)$, and $R_\unit(t)$, respectively.
The population dynamics are written in terms of the counting processes $N_{Q_1Q_2,\unit}(t)$ enumerating the cumulative transitions in town $\unit$, up to time $t$, from compartment $Q_1$ to $Q_2$.
Here, $Q_1,Q_2\in \{S,E,I,R,B,D\}$, with $B$ denoting a source compartment for immigration or birth, and $D$ denoting a sink compartment for emigration or death.
We enumerate the $U=20$ towns studied by \citet{he10} in decreasing size, so that $\unit=1$ corresponds to London.
Our model is described by the following system of stochastic differential equations, for $\unit=1,\dots, \Unit$:
\begin{equation}
\nonumber
%\label{eq:measles:system}
\begin{array}{lllllll}
\displaystyle dS_\unit(t) &=& dN_{BS,\unit}(t) &-& dN_{SE,\unit}(t) &-& dN_{SD,\unit}(t), \\
\displaystyle dE_\unit(t) &=& dN_{SE,\unit}(t) &-& dN_{EI,\unit}(t) &-& dN_{ED,\unit}(t), \\
\displaystyle dI_\unit(t) &=& dN_{EI,\unit}(t) &-& dN_{IR,\unit}(t) &-& dN_{ID,\unit}(t).
\end{array}
\end{equation}
The total population $P_\unit(t)=S_\unit(t)+E_\unit(t)+I_\unit(t)+R_\unit(t)$ is calculated by smoothing census data, and is treated as known.
The number of recovered individuals $R_\unit(t)$ in town $\unit$ is therefore defined implicitly.
Furthermore, $N_{SE,\unit}(t)$ is modeled as a negative binomial death process \citep{breto09,breto11}, defined by a rate, $\mu_{SE,\unit}(t)$, specified as
\begin{equation}
\nonumber
\mu_{SE,\unit}=
\beta_{\unit}
\left[
  \left( \frac{I_\unit+\iota_u}{P_\unit} \right)^{\alpha_{\unit}}
\hspace{-1mm}
 + \sum_{\altUnit \neq \unit} \frac{v_{\unit\altUnit}}{P_\unit}
  \left\{
    \left(
      \frac{ I_{\altUnit} }{ P_{\altUnit} }
    \right)^{\alpha_{\unit}} -
    \hspace{-1mm}
    \left(
      \frac{I_\unit}{P_\unit}
    \right)^{\alpha_{\unit}}
  \right\}
\right] \frac{d\Gamma_{SE,\unit}}{dt}.
%\label{eqn:transmissionrate}
\end{equation}
Here, the time dependence of $\mu_{SE,u}$, $\beta_u$, $I_u$, and $P_u$ is suppressed;
$\alpha_{\unit}$ is a mixing exponent modeling inhomogeneous contact rates within a town;
$\iota_{\unit}$ models immigration of infected individuals;
${d\Gamma_{SE,\unit}}/{dt}$ is gamma white noise, with intensity parameter $\sigma_{SE.\unit}$;
and $\beta_{\unit}$ models the seasonal transmission driven by high contact rates between children at school,
\begin{equation}
\nonumber
  \beta_{\unit}(t)=\Bigg\{
  \begin{array}{ll}
\big(1+\amplitude_{\unit}(1-\schoolTermFrac)\schoolTermFrac^{-1} \big)\, \meanBeta_{\unit} & \mbox{ during school term},\\
\big( 1-\amplitude_\unit\big) \, \meanBeta_{\unit} & \mbox{ during vacation},
  \end{array}
\label{eq:term}
\end{equation}
where $\schoolTermFrac = 0.759$ is the proportion of the year taken up by the school terms, $\meanBeta_\unit$ is the mean transmission rate, and $\amplitude_{\unit}$ measures the reduction in transmission during school holidays.

The number of travelers from town $\unit$ to $\altUnit$ is denoted by $v_{\unit\altUnit}$.
Here, $v_{\unit\altUnit}$ is constructed using the gravity movement model of \cite{xia04}, given by
\begin{equation}
\nonumber
v_{\unit\altUnit} = \gravity_{\unit} \cdot \frac{\hspace{2mm} \overline{\dist}\hspace{2mm}}{\hspace{1.5mm}\overline{\pop}^2\hspace{1mm}} \cdot \frac{\pop_\unit \cdot \pop_{\altUnit}}{\dist_{\unit\altUnit}},
\end{equation}
where $\gravity_u$ is the {\it gravitational constant}, $\dist_{\unit\altUnit}$ is the distance between town $\unit$ and town $\altUnit$, $\pop_\unit$ is the average of $P_{\unit}(t)$ over time, $\overline{\pop}$ is the average of $\pop_{\unit}$ across towns, and $\overline{\dist}$ is the average of $\dist_{\unit\altUnit}$ across all pairs of towns.
The transition processes $N_{EI,\unit}(t)$, $N_{IR,\unit}(t)$, and $N_{QD,\unit}(t)$, for $Q\in\{S,E,I,R\}$,  are modeled as conditional Poisson processes with per-capita rates $\mu_{EI,\unit}$, $\mu_{IR,\unit}$, and $\mu_{QD,\unit}$, respectively, and we fix $\mu_{QD,\unit}=(50 \mbox{ year})^{-1}$.
A fraction $\cohort$ of births enter the susceptible cohort on the school admission day, and hence the birth process $N_{BS,\unit}(t)$ is an inhomogeneous Poisson process with rate
$\mu_{BS,\unit}(t-\birthdelay) \big[
  (1-\cohort) + \cohort \delta(t-t_a) \big]$,
where $\mu_{BS,\unit}(t)$ is specified by interpolated census data, $t_a=\lfloor t \rfloor + 252/365$ is the admission date for the year containing $t$, $\delta$ is a Dirac delta function, and $\birthdelay=4\, \mbox{ year}$ is a fixed delay between birth and entry into a high-transmission school community.

To describe the measurement process, let $Z_{\unit\comma\time}=N_{IR\comma\unit}(t_\time)-N_{IR\comma\unit}(t_{\time-1})$ be the number of removed infected individuals in the $n$th reporting interval.
Suppose that cases are quarantined once they are identified, so that reported cases comprise a fraction $\rho$ of these removal events.
The case report $\data{y}_{\unit\comma\time}$ is modeled as a realization of a discretized conditionally Gaussian random variable $Y_{\unit\comma\time}$, defined for $y>0$ via
\begin{eqnarray}
\nonumber
\prob\big[Y_{\unit\comma\time}{=}y\mid Z_{\unit\comma\time}{=}z\big] &=& \varPhi\big(y+0.5; \rho_{\unit} z,\rho_{\unit}(1-\rho_{\unit})z+\measOD_{\unit}^2\rho_{\unit}^2z^2\big)
\\
&&\hspace{-20mm}
- \varPhi\big(y-0.5; \rho_{\unit} z,\rho_{\unit}(1-\rho_{\unit})z+\measOD_{\unit}^2\rho^2z^2\big),
\label{eq:obs}
\end{eqnarray}
where $\varPhi(\cdot;\mu,\sigma^2)$ is the $\normal(\mu,\sigma^2)$ cumulative distribution function, and $\measOD$ models overdispersion relative to the binomial distribution.
For $y=0$, we replace $y-0.5$ with $-\infty$ in \eqref{eq:obs}.
Three data points are treated as missing by \citet{he10}, owing to presumed recording errors.
We follow this decision, implemented by including a special value $\mathrm{NA}$ in $\Yspace$, and setting $Y_{\unit,n}$ to $\mathrm{NA}$ with probability one when $\data{y}_{\unit,n}$ is missing.

We have written the model with all parameters unit-specific, so $\theta=\psi_{1:U}$ with
\arxiv{\begin{equation*}}
\statSinica{$}
\psi_u=\big(\meanBeta_{\unit},\mu_{EI,\unit}, \mu_{IR,\unit},\rho_{\unit},\measOD_{\unit},\sigma_{SE,\unit},\gravity_{\unit},\iota_{\unit},\amplitude_{\unit},\alpha_{\unit}, S_{0,\unit}, E_{0,\unit},I_{0,\unit}\big),
\arxiv{\end{equation*}}
\statSinica{$}
for $u\in\seq{1}{U}$.
This defines an extended model for implementing an IBPF with shared parameters, as in \eqref{eq:extension}.
It also serves another function by allowing us address a key data analysis question of which parameters should be shared between units and which should be unit-specific.



<<sim-plot, echo=F, fig.height=6.5, fig.width=6.5, out.width="5.5in", fig.cap = "Simulated weekly measles case reports.",fig.show = "hold">>=
library(spatPomp)
x <- sim <- readRDS("sim/sim.rds")
df <- as.data.frame(x)
df[x@unit_obsnames] <- log10(df[x@unit_obsnames]+1)
unit_nm <- rlang::sym(x@unitname)
df[[unit_nm]] <- factor(df[[unit_nm]], levels = x@unit_names)
g <- ggplot2::ggplot(data = df,
  mapping = ggplot2::aes(x = !!rlang::sym(x@timename),
    y = !!rlang::sym(x@unit_obsnames))) +
  ggplot2::geom_line() +
  ggplot2::facet_wrap(unit_nm)+
  ggplot2::ylab("log10(simulated cases)")+
  ggplot2::xlab("Year")+
  theme(text=element_text(family="serif"))
print(g)
@

The data are shown in Fig~\ref{fig:data-plot}, and simulations from the model are shown in Fig~\ref{fig:sim-plot}.
Parameters for the simulated model were based on the analysis of individual towns by \citet{he10}.
In order to investigate estimation of either shared or unit-specific parameters, we conduct the simulation with all parameters being shared.
To find a shared parameter vector capable of providing a reasonable representation of all the towns simultaneously, we experimented to look for a visual match between the data and the simulations.
The parameters used are reported in the Supplementary Material (Section~\supSecMeaslesParams).

Estimated parameter values may have scientific interest, but we focus on the statistical task of likelihood maximization.
In the presence of weak identifiability, small differences in likelihood could lead to large differences in parameter estimates.
In such situations, a scientist may choose to investigate what functions of the parameters can be inferred accurately without adding additional assumptions.
Alternatively, they may choose to investigate the consequences of placing constraints on some parameters in order to improve the identifiability of the remainder.
Likelihood maximization permits such investigations, but is beyond the scope of this study.
In the Supplementary Material, we provide the parameter values used for the simulation study and those obtained by the likelihood maximization.

To facilitate data analysis, our methodology needs to operate across the full spectrum of decisions on shared versus unit-specific parameter designations.
We therefore test our method on three submodels: {\modelA} has mostly shared parameters, with only the initial values and the reporting rate unit-specific; {\modelB} has every parameter unit-specific; and {\modelC} has every parameter unit-specific, and the dynamic coupling (in our context, the movement of infected individuals between towns) is replaced with an external forcing of each unit (in our context, the immigration rate of infected individuals from outside the study population).
Section~{\supSecMeaslesParams} defines the submodels in further detail.
Submodel {\modelC} provides a useful point of reference, because it is a special case of a PanelPOMP model \citep{breto19}, and also can be analyzed as a collection of separate POMP models.
We set up our model so that {\modelC} matches the analysis of \citet{he10}.
One may expect that methods that take advantage of the special structure of {\modelC} should outperform more general methods that permit coupling between units.
Thus, we expect the SpatPOMP methods to be less efficient numerically than application of POMP methods separately to each unit.
Here we answer the following questions: How much less efficient are the SpatPOMP methods? Are simulation-based SpatPOMP inference methods practical for situations such as the measles model of \citet{he10}?


The simulated model is drawn from model {\modelA}, which is nested within model {\modelB}, but not within {\modelC}.
For models {\modelB} and {\modelC}, in which all parameters are unit-specific, we must estimate $20\times 13=260$ parameters.
This greatly exceeds the seven shared parameters fitted by \citet{ionides21} for a measles metapopulation model, which required significant computational effort.
An iterated guided intermediate resampling filter (IGIRF) algorithm was used to fit nine shared parameters and three unit-specific initial value parameters in a measles metapopulation model \citep{park20}.
The latter analysis uses a customized treatment for the unit-specific initial value parameters, and does not attempt to estimate other unit-specific parameters.
IGIRF is sensitive to the choice of guide function, and the model-specific implementation of \citet{park20} outperforms the current generic implementation of IGIRF in \code{spatPomp}.
Thus, to the best of our knowledge, the {\ibpf} presented in Algorithm~\ref{alg:ibpfilter} advances the current limits on the scalability of simulation-based maximum-likelihood inference for metapopulation dynamics.

<<sim-loglik-truth,echo=FALSE>>=
readRDS("w/w1_3/lik.rds") -> w
w6400 <- w[w[,"J"]==6400,]
loglik_truth <- logmeanexp(w6400[,"logLik"],se=TRUE)
@

Before engaging in likelihood maximization, we first validate the likelihood evaluation; see \citet{ionides21} for likelihood evaluation for metapopulation models.
Briefly, the basic particle filter provides a consistent evaluation of the log-likelihood in a limit with sufficient particles to make the Monte Carlo standard deviation small.
This is practical only when $U$ is small (say, $U\le 5$), but this situation can be used to calibrate the bias induced by a BPF, which turns out to be small for our multi-town measles model when each town is its own block.
It may be surprising that resampling independently on each block (which is what a BPF does) is able to capture the dependence.
Heuristically, note that the dynamic dependence between blocks is maintained by a BPF, which updates particles according to the full coupled dynamics.
Whether this is sufficient to obtain a good approximation to the filter distribution depends on the situation, but for the specific case of metapopulation models (for which the strongest coupling is within units, rather than between units), the approximation can be empirically successful.
In principle, the approximation error can be reduced by increasing the number of units in each block.
However, in practice, the additional Monte Carlo variance acquired by doing so is not worthwhile when the coupling is relatively weak.
The BPF log-likelihood evaluation at the true parameter value is $\loglik_{\mathrm{true}}=\Sexpr{myround(loglik_truth[1],1)}$, with a Monte Carlo standard error of $\Sexpr{myround(loglik_truth[2],1)}$.


\subsection{IBPF on simulated data}
\label{sec:sim}


<<v4_start,echo=F>>=
readRDS("v4/v1a_3/initial_lik.rds") -> loglik_start
median_start <- median(loglik_start[,"start_loglik"])
@

<<sim_results,echo=F,results=F>>=
library(pomp)
truth_eval <- readRDS("w/w1_3/lik.rds")
truth_loglik<-logmeanexp(truth_eval[truth_eval[,"J"]==25600,"logLik"])
v1a <- read.csv(file="v4/v1a_3/global.csv")
v1b <- read.csv(file="v4/v1b_3/global.csv")
v1c <- read.csv(file="v4/v1c_3/global.csv")
v2a <- read.csv(file="v4/v2a_3/global.csv")
v2b <- read.csv(file="v4/v2b_3/global.csv")
v2c <- read.csv(file="v4/v2c_3/global.csv")
v3a <- read.csv(file="v4/v3a_3/global.csv")
v3b <- read.csv(file="v4/v3b_3/global.csv")
v3c <- read.csv(file="v4/v3c_3/global.csv")
v4a <- read.csv(file="v4/v4a_3/global.csv")
v4b <- read.csv(file="v4/v4b_3/global.csv")
v4c <- read.csv(file="v4/v4c_3/global.csv")
v5a <- read.csv(file="v4/v5a_3/global.csv")
v5b <- read.csv(file="v4/v5b_3/global.csv")
v5c <- read.csv(file="v4/v5c_3/global.csv")
@

<<z3_results,echo=F,results=F>>=
z1a <- read.csv(file="z3/z1a_3/global.csv")
z1b <- read.csv(file="z3/z1b_3/global.csv")
z1c <- read.csv(file="z3/z1c_3/global.csv")
z1d <- read.csv(file="z3/z1d_3/global.csv")
z2a <- read.csv(file="z3/z2a_3/global.csv")
z2b <- read.csv(file="z3/z2b_3/global.csv")
z2c <- read.csv(file="z3/z2c_3/global.csv")
z2d <- read.csv(file="z3/z2d_3/global.csv")
z3a <- read.csv(file="z3/z3a_3/global.csv")
z3b <- read.csv(file="z3/z3b_3/global.csv")
z3c <- read.csv(file="z3/z3c_3/global.csv")
z3d <- read.csv(file="z3/z3d_3/global.csv")
z4a <- read.csv(file="z3/z4a_3/global.csv")
z4b <- read.csv(file="z3/z4b_3/global.csv")
z4c <- read.csv(file="z3/z4c_3/global.csv")
z4d <- read.csv(file="z3/z4d_3/global.csv")
z5a <- read.csv(file="z3/z5a_3/global.csv")
z5b <- read.csv(file="z3/z5b_3/global.csv")
z5c <- read.csv(file="z3/z5c_3/global.csv")
z5d <- read.csv(file="z3/z5d_3/global.csv")
loglik_he <- sum(read.csv("data/he10mle.csv")$loglik)
# -40345.7
@

<<sim_search_boxplot, echo=F, fig.height=3, fig.width=5.5, out.width="5in", fig.cap = "Fitting simulated measles data, using an initial search and four refinement steps. (A1--A5) model \\modelA, $4\\times 20$ unit-specific parameters and nine shared parameters; (B1--B5) model \\modelB, $13\\times 20$ unit-specific parameters and no shared parameters;  (C1--C5) model \\modelC, all unit-specific, but with immigration rather than coupling, matching He et al.~(2010). The horizontal dashed line is the log-likelihood at the true parameters, evaluated using BPF.",fig.show = "hold">>=
a <- cbind(
A1 = v1a$loglik,
A2 = v2a$loglik,
A3 = v3a$loglik,
A4 = v4a$loglik,
A5 = v5a$loglik
)

b <- cbind(
B1 = v1b$loglik,
B2 = v2b$loglik,
B3 = v3b$loglik,
B4 = v4b$loglik,
B5 = v5b$loglik
)

c <- cbind(
C1 = v1c$loglik,
C2 = v2c$loglik,
C3 = v3c$loglik,
C4 = v4c$loglik,
C5 = v5c$loglik
)

par(mai=c(0.4,0.6,0.1,0.1))
par(family="serif")
loglik_max <- max(cbind(a,b,c),truth_loglik)
boxplot(cbind(a,b,c),ylim=loglik_max-c(200,0),
ylab="Log-likelihood")
abline(h=truth_loglik,lty="dashed",col="blue")
@


One of our goals is to obtain appropriate algorithmic choices for a data analysis.
Thus, we seek to develop methodology that is demonstrably successful when the truth is known, before applying it to data.
One could revisit the simulation study based on the data analysis in Section~\ref{sec:data} using MLEs of the parameters.

In Figure~\ref{fig:sim_search_boxplot} we investigate a sequence of successive searches for the MLE for the models {\modelA}, {\modelB}, and {\modelC} described previously.
Each search is replicated $36$ times.
Search~1 was started with each parameter adjusted by a uniform $[-0.1,0.1]$ random perturbation on an appropriate dimensionless scale (log for nonnegative parameters, and logit for $[0,1]$ valued parameters; see Section~{\supSecAlg}).
This is a fairly small perturbation, but it is nevertheless sufficient to knock the likelihood of the parameter vector around 200 log units below the MLE (shown in Figure~\ref{fig:search_diagnostics}).
Our goal is to show that the algorithm can succeed reliably on a relatively easy local optimization task.
Subsequent searches were started with four copies of each parameter vector with an estimated likelihood in the top 25\% for the previous search.

The MLE is not known exactly in this case.
Wilks' theorem gives an asymptotic expectation that the log-likelihood at the MLE should be greater than the likelihood at the truth by approximately $1/2$ the number of parameters, which here is $(4\times 20+9)/2=44.5$ for {\modelA}, and $(13\times 20)/2=130$ for {\modelB}.
The $y$-axis values in Figure~\ref{fig:sim_search_boxplot} show log-likelihoods exceeding the truth by less than this, indicating some imperfection in the Monte Carlo maximization so far as Wilks' asymptotic result holds.
Despite this limitation, searches that exceed the likelihood at the truth have found inferentially plausible sets of parameters that can be used to study the likelihood surface around the maximum.
For example, Monte Carlo profile confidence intervals can give proper coverage, even in the presence of considerable Monte Carlo error \citep{ionides17,ning21}.

<<search_diagnostics, echo=F, fig.height=6.5, fig.width=4.5, out.width="5in", fig.cap = "Three steps of a likelihood search on simulated data (left panel, model \\modelA) and UK measles data (right panel, model \\modelB). Dashed lines parallel to the axes show the log-likelihood at the truth (left panel) and the \\citet{he10} value (right panel). Points above the diagonal dashed line show improvement owing to the search step.">>=
par(mfcol=c(3,2))
par(mai=c(0.3,0.3,0.1,0.1))
par(omi=c(0.3,0.5,0.5,0.1))
par(family = "serif")
##
## (1,1)
##
x <- v1a[,"start_loglik"]
y <- v1a$loglik
x_lim <- range(c(x,truth_loglik,max(y)))
y_lim <- range(c(y,truth_loglik,max(x)))
plot(x,y,xlim=x_lim,ylim=y_lim,
  xlab="",
  ylab="Initial search",
)
abline(a=0,b=1,lty="dashed",col="blue")
abline(h=truth_loglik,lty="dashed",col="blue")
abline(v=truth_loglik,lty="dashed",col="blue")
##
## (2,1)
##
x <- rep(v1a$loglik[1:18],each=2)
y <- v2a$loglik
x_lim <- range(c(x,truth_loglik,max(y)))
y_lim <- range(c(y,truth_loglik,max(x)))
plot(x,y,xlim=x_lim,ylim=y_lim,
  xlab="",
  ylab="1st refinement"
)
abline(a=0,b=1,lty="dashed",col="blue")
abline(h=truth_loglik,lty="dashed",col="blue")
abline(v=truth_loglik,lty="dashed",col="blue")
##
## (3,1)
##
x <- rep(v2a$loglik[1:18],each=2)
y <- v3a$loglik
x_lim <- range(c(x,truth_loglik,max(y)))
y_lim <- range(c(y,truth_loglik,max(x)))
plot(x,y,xlim=x_lim,ylim=y_lim,
    ylab="2nd refinement",
    xlab="Starting log-likelihood"
)
abline(a=0,b=1,lty="dashed",col="blue")
abline(h=truth_loglik,lty="dashed",col="blue")
abline(v=truth_loglik,lty="dashed",col="blue")
##
##
## (1,2)
##
  x <- z1b[,"start_loglik"]
  y <- z1b$loglik
  x_lim <- range(c(x,loglik_he,max(y)))
  y_lim <- range(c(y,loglik_he,max(x)))
  plot(x,y,xlim=x_lim,ylim=y_lim,
    xlab="",
    ylab="",
  )
  abline(a=0,b=1,lty="dashed",col="blue")
  abline(h=loglik_he,lty="dashed",col="blue")
  abline(v=loglik_he,lty="dashed",col="blue")
##
## (2,2)
##
  x <- rep(z1b$loglik[1:18],each=2)
  y <- z2b$loglik
  x_lim <- range(c(x,loglik_he,max(y)))
  y_lim <- range(c(y,loglik_he,max(x)))
  plot(x,y,xlim=x_lim,ylim=y_lim,
    xlab="",
    ylab=""
  )
  abline(a=0,b=1,lty="dashed",col="blue")
  abline(h=loglik_he,lty="dashed",col="blue")
  abline(v=loglik_he,lty="dashed",col="blue")
##
## (3,2)
##
  x <- rep(z2b$loglik[1:18],each=2)
  y <- z3b$loglik
  x_lim <- range(c(x,loglik_he,max(y)))
  y_lim <- range(c(y,loglik_he,max(x)))
  plot(x,y,xlim=x_lim,ylim=y_lim,
    xlab="Starting log-likelihood",
    ylab=""
  )
  abline(a=0,b=1,lty="dashed",col="blue")
  abline(h=loglik_he,lty="dashed",col="blue")
  abline(v=loglik_he,lty="dashed",col="blue")

LINE<-0.25
CEX <- 0.9
mtext("Initial search",side=2,line=LINE,outer=TRUE,at=0.833,cex=CEX)
mtext("First refinement",side=2,line=LINE,outer=TRUE,at=0.5,cex=CEX)
mtext("Second refinement",side=2,line=LINE,outer=TRUE,at=0.167,cex=CEX)
mtext("Simulated data",side=3,line=LINE,outer=TRUE,at=0.25,cex=CEX)
mtext("UK towns",side=3,line=LINE,outer=TRUE,at=0.75,cex=CEX)
mtext("Log-likelihood",side=1,line=LINE,outer=TRUE,at=0.25,cex=CEX)
mtext("Log-likelihood",side=1,line=LINE,outer=TRUE,at=0.75,cex=CEX)

@


Figure~\ref{fig:search_diagnostics} shows the convergence diagnostics corresponding to {\modelA} for the simulated data (the first column) and {\modelB} for the UK measles data (the second column).
For now, we focus on the first column.
The first row plots the log-likelihood obtained after the initial search against the log-likelihood of the randomly selected starting value.
The horizontal and vertical dashed lines denote the likelihood at the truth, and the diagonal dashed line represents equality, so that points above the diagonal show improvement after the search.
We find that {\ibpf} robustly and rapidly approaches a neighborhood of the MLE, as measured by likelihood.
The second row shows that further investigation of more successful searches can reliably obtain likelihood values higher than those at the truth.
However, for a Monte Carlo search based on a Monte Carlo likelihood evaluation, it may be problematic to pinpoint the exact maximum in a high-dimensional space.
The third row shows that continued searching does not lead to substantially better outcomes.
When using these methods, we emphasize the need to make proper inferences despite imperfect maximization \citep{ionides17,ning21}.


The results in Figure~\ref{fig:sim_search_boxplot} and the first column of  Figure~\ref{fig:search_diagnostics} show that {\ibpf} can be effective for simulated data, but do not guarantee comparable performance for our data analysis.
Indeed, model misspecification, which is inevitable for data analysis, may be expected to add difficulties to filtering and therefore to numerical methods based on filtering.
Rather, we view the simulation study as a lower bound on the effort required to perform effective inference on data, and therefore a starting point for investigating how to proceed with a data analysis.
Before moving on to the data analysis, we briefly describe the details of the model.


\subsection{IBPF applied to data}
\label{sec:data}


<<data_search_boxplot, echo=F,  fig.height=3, fig.width=5.5, out.width="5in", fig.cap = "Fitting different models to the UK measles data using the method tested on simulated data. (A) $4\\times 20$ unit-specific parameters and nine shared parameters; (B) $13\\times 20$ unit-specific parameters and no shared parameters; (C) all unit-specific, but with immigration rather than coupling. The horizontal dashed line is the likelihood from \\citet{he10}.">>=
a <- cbind(
A1 = z1a$loglik,
A2 = z2a$loglik,
A3 = z3a$loglik,
A4 = z4a$loglik,
A5 = z5a$loglik
)

b <- cbind(
B1 = z1b$loglik,
B2 = z2b$loglik,
B3 = z3b$loglik,
B4 = z4b$loglik,
B5 = z5b$loglik
)

c <- cbind(
C1 = z1c$loglik,
C2 = z2c$loglik,
C3 = z3c$loglik,
C4 = z4c$loglik,
C5 = z5c$loglik
)

d <- cbind(
D1 = z1d$loglik,
D2 = z2d$loglik,
D3 = z3d$loglik,
D4 = z4d$loglik,
D5 = z5d$loglik
)
loglik_max <- max(cbind(a,b,c),loglik_he)

par(mai=c(0.5,0.8,0.1,0.1))
par(family="serif")
boxplot(cbind(a,b,c),ylim=loglik_max-c(5500,0),
  ylab="Log-likelihood")
abline(h=loglik_he,lty="dashed",col="blue")
@



Figure~\ref{fig:data_search_boxplot} shows the results from fitting models {\modelA}, {\modelB}, and {\modelC} to the UK measles data using successive rounds of {\ibpf}, applying the same method used for the simulation study shown in Figure~\ref{fig:sim_search_boxplot}.
The dashed lines show the sum of the log-likelihoods obtained by \citet{he10}, $\ell_{\mathrm He}=\Sexpr{myround(loglik_he,1)}$.
The value $\ell_{\mathrm He}$ corresponds to model {\modelC}.
BPF with each town forming a separate block, applied to an uncoupled model such as {\modelC}, is equivalent to carrying out independent particle filters for each unit.
Indeed, if we assign the published parameter values from \citet{he10} to the \code{spatPomp} object for model {\modelC}, and apply the \code{bpfilter} function to carry out BPF, we retrieve $\ell_{\mathrm He}$, up to a Monte Carlo error.
Models {\modelA}, {\modelB}, and {\modelC} in Figure~\ref{fig:data_search_boxplot} each show a shortfall relative to $\ell_{\mathrm He}$.

The largest shortfall is for {\modelA}, perhaps because {\modelA} has more shared parameters than the evidence in the data supports.
The results of \citet{he10} suggest that the data are explained better when various parameters are a function of the town population.
However, determining suitable functional forms for this relationship, and establishing regularities across towns that can be represented by shared parameters, remains an open problem.
This may be investigated using panel methods, such as PanelPOMP models \citep{breto19}, in addition to consideration of SpatPOMP models.

Models {\modelB} and {\modelC} yield comparable likelihoods, which is in contrast to the results shown in Figure~\ref{fig:sim_search_boxplot}.
Fitting to simulated data from {\modelB} (in the special case in which all unit-specific parameters are equal across all units), the shortfall for {\modelC} in Figure~\ref{fig:sim_search_boxplot} indicates that we obtain a substantially worse fit when we approximate coupling using an uncoupled reservoir of infection.
If the actual data were also explained substantially better by the coupled model, we would expect to see comparable results in the data analysis.
However, because we do not, we conclude that this coupled model does not provide a substantially better explanation than the uncoupled model.
Although various other candidate coupling mechanisms have been proposed \citep{bjornstad19}, they have not yet been fitted to the full data, suggesting a lack of appropriate methodology to do so.
Instead, \citet{bjornstad19} consider summary statistics based on local fade-outs and re-introductions.


Figure~\ref{fig:data_search_boxplot} shows that model {\modelB} has a small, but distinct shortfall compared with $\ell_{\mathrm He}$.
For the simulated data, we do not observe this shortfall, and thus we deduce that the real data provide a more challenging optimization environment.
When performing difficult optimization problems, it may be possible to develop helpful strategies specific to the model and data in question.
For example, one could try merging unit-specific parameters from different searches, using the likelihood component for each unit to assess successful choices.
However, optimization heuristics such as this do not have general theoretical support; if they obtain higher likelihoods, that is sufficient justification.
For analyses of panel time series data using PanelPOMP models, such methods have clearer theoretical support and have been found to be useful \citep{breto19}.

\section{Discussion}
\label{sec:discussion}

Theoretical interest in BPF algorithms has been inspired by the work of \citet{rebeschini15}, who showed mathematically that a BPF enjoys linear scaling properties under suitable conditions.
However, they expressed pessimism about the practical applicability of their algorithm, which may help to explain its limited practical use.
A similar algorithm was proposed independently  under the name of a factored particle filter by \citet{ng02}, who offered an empirical justification.
However, the latter algorithm has also seen only limited use.
The numerical results in Figure~3 of \citet{ionides21} suggest that BPF is particularly well suited to metapopulation models, in which we expect most of the population dynamics to occur at a local level, among individuals at one spatial unit.
As a broad generalization, in ecological systems, the dispersion of individuals between spatial units is rare but dynamically important.
Therefore, edge effects between blocks, which may be a serious problem for a BPF in a system with stronger spatial coupling \citep{ionides21} is a relatively minor concern in metapopulation models.
Blocks of size one unit are therefore a natural choice for block filtering of metapopulation models when the constituent populations are spatially distinct.

We have proposed a likelihood-maximization approach to inference.
Much research has been done on inference for high-dimensional partially observed stochastic dynamic systems, and we have cited only the most directly relevant work.
A Bayesian inference approach based on the ensemble Kalman filter is proposed by \citet{katzfuss19}.
An expectation-maximization approach based on a BPF is presented by \citet{finke17}.
Spatiotemporal models with a convenient factorization across units are studied by \citet{beskos17} and \citet{xu19}.

  We have demonstrated a workflow that led to a likelihood-based assessment of measles metapopulation models ({\modelA} and {\modelB}), with the possibility of finding evidence that they outperform the uncoupled model, {\modelC}.
  Our methodology successfully refutes {\modelC} on simulated data, when the truth is within {\modelA} and {\modelB}.
  However, there was no evidence of an advantage for these coupled models when the same comparison was carried out on the data.
Likelihood maximization for the measles data fits the {\it common task framework} described by \citet[][Section~6]{donoho17}, with the likelihood value for {\modelC} obtained by \citet{he10} providing a benchmark challenge.
Future improvements in models, perhaps facilitated by the open-source models and methods accompanying this article, may obtain metapopulation models that convincingly beat model {\modelC}.

The computation time for Figures~\ref{fig:sim_search_boxplot} and~\ref{fig:data_search_boxplot} was approximately 24hr on a single core of a computing node, for 4000 particles iterated 100 times over 730 time points (weekly data for 14 years) for 20 cities.
Each box in these figures involves 36 replications, and thus took 24 hours on all cores of one node on a computing cluster.
A measure of computational efficiency is the size of a problem that can be solved on this timescale, as discussed further in Section~{\supSecEfficiency}.
Tasks that are considerably larger, perhaps $10^3$ or more spatial units, may require additional approximations, such as those inherent in the ensemble Kalman filter \citep{evensen09book,katzfuss19} or other numerical filtering techniques \citep{whitehouse22}.
However, we anticipate that many practical metapopulation analyses can be addressed within the scope we have demonstrated.
The majority of the computational effort lies in simulating from the dynamic model.
In the \code{spatPomp} R package, as in \code{pomp}, the user supplies a snippet of C code to simulate a single particle between a single arbitrary pair of times, and the package provides a vectorized form of this computation, which is used by the inference methodologies within the package.
As a result, the computational performance is competitive even though the majority of the package is written in R \citep[Table~1]{FitzJohn20}.

\vspace{3mm}

%\newpage

\noindent {\bf Supplementary material}

\vspace{1mm}

\noindent Supplementary text presents the following: S1, parameters for the measles model; S2, algorithmic parameters and transformations; S3, computational efficiency; S4, varying the spatial autoregression parameter, $r$. Source code is at \url{https://github.com/ionides/ibpf_article}.

\vspace{3mm}

%\newpage

\noindent {\bf Acknowledgments}

\vspace{1mm}

\noindent This work was supported by National Science Foundation grants DMS-1761603 and DMS 1646108, and a Texas A\&M University Seed Fund Grant.
We acknowledge an anonymous referee from a previous article who recommended implementing a block particle filter.
We are also grateful for constructive feedback from two anonymous referees.

\arxiv{
\vspace{2mm}
\noindent{\bf Version notes}

\vspace{1mm}

\noindent Version [v1] was submitted to {\it Statistica Sinica}, and [v2] was accepted for publication. [v3] corrects for an omission of the cohort effect in the description of the model in Section~\ref{sec:model}.
}

\bibliography{bib-ibpf}
%\InputIfFileExists{ms.bbl}


\arxiv{
\newpage

\setcounter{section}{0}
\setcounter{equation}{0}
\def\theequation{S\arabic{section}.\arabic{equation}}
%\def\thesection{S\arabic{section}}
\renewcommand\thefigure{S\arabic{figure}}
\renewcommand\thetable{S\arabic{table}}
\def\thesection{S\arabic{section}}

<<read-sim-model,echo=F,eval=T,results="hide",warning=FALSE,message=FALSE>>=
library(spatPomp)
po <- readRDS("sim/he10.rds")
theta <- coef(po)
myround<- function (x, digits = 1) {
  # adapted from the broman package
  # solves the bug that round() kills significant trailing zeros
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  if (digits < 1) {
    as.character(round(x,digits))
  } else {
    tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
    zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
    tmp[tmp == paste0("-", zero)] <- zero
    tmp
  }
}

mysignif <- function (x, digits = 1) {
  myround(x, digits - ceiling(log10(abs(x))))
}
@

\section{Parameters for the measles model}

\newcommand\measlesShared{shared}
\newcommand\measlesUnitSpecific{unit-specific}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Parameter & \modelA & \modelB & \modelC & Simulation \\
\hline
$\pinit_{S,u}$ & \measlesUnitSpecific & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["S_01"],3)}$ \\
$\pinit_{E,u}$ & \measlesUnitSpecific & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["E_01"],5)}$ \\
$\pinit_{I,u}$ & \measlesUnitSpecific & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["I_01"],5)}$ \\
$\rho_u$ & \measlesUnitSpecific & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["rho1"],1)}$ \\
$\measOD_{\unit}$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["psi1"],2)}$ \\  
$\mu_{EI,\unit}$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["sigma1"]/52,0)} \, \mathrm{week}^{-1}$ \\
$\mu_{IR}$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["gamma1"]/52,0)} \, \mathrm{week}^{-1}$ \\
$\meanBeta_\unit$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["gamma1"]*(theta["R01"]+theta["mu1"])/52,0)} \, \mathrm{week}^{-1}$ \\
$\sigma_{SE,\unit}$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["sigmaSE1"],2)} \, \mathrm{year}^{1/2}$ \\
$\amplitude_{\unit}$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["amplitude1"],1)}$ \\
$\alpha_{\unit}$ & \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["alpha1"],0)}$ \\
$\cohort$& \measlesShared & \measlesUnitSpecific & \measlesUnitSpecific &
  $\Sexpr{myround(theta["cohort1"],0)}$ \\  
$\gravity_{\unit}$ & \measlesShared & \measlesUnitSpecific & 0 &
  $\Sexpr{myround(theta["g1"],0)}$ \\
$\iota_{\unit}$ & 0 & 0 & \measlesUnitSpecific &
  0 \\
\hline
\end{tabular}
\caption{Parameters for the measles model. Sub-model {\modelA} has 4 unit-specific parameters and 9 shared parameters, with movement between units following a gravity equation. Sub-model {\modelB} has 13 unit-specific parameters and no shared parameters, with gravity movement. Sub-model {\modelC} has 13 unit-specific parameters and no shared parameters, with independent immigration of infections rather than movement between units. The last column shows the parameter values used for the simulated data in Figure~2 of the main text.
}\label{tab:params}
\end{center}
\end{table}

\noindent Notes on Table~\ref{tab:params}:
\begin{enumerate}
\item In the model formulation, all parameters are written as unit-specific, i.e., with a $u$ subscript. Shared parameters take equal values across all units.
\item For the simulation, all parameters are shared, so one can assess the inferential consequences of estimating models with some or all parameters unit-specific. Establishing a good choice of parameters to be shared or unit-specific for the data is a data analysis goal.
\item The initial values of the latent states are parameterized as fractions of the total population, so
$S_{0,u}=\pinit_{S,u}P_u(t_0)$,
$E_{0,u}=\pinit_{E,u}P_u(t_0)$,
$I_{0,u}=\pinit_{I,u}P_u(t_0)$,
and $R_{0,u}=\big(1-\pinit_{S,u}-\pinit_{E,u}-\pinit_{I,u}\big)P_u(t_0)$.
\item The parameterization used in the numerical implementation replaces $\meanBeta_{\unit}$ with the basic reproductive number, a dimensionless ratio defined as
$\mathscr{R}_{0,\unit}= \meanBeta_{\unit} \big(\mu_{IR,\unit}+\mu_D\big)^{-1}$.
For the simulation,  $\mathscr{R}_{0,\unit}=\Sexpr{myround(theta["R01"],0)}$.
\end{enumerate}

%\newpage

\section{Algorithmic parameters and transformations}


%% values taken from z3/z1a.r and v4/v4a.r


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Parameter & Value \\
\hline
$J$ & 4000 \\
$M$ & 100 \\
$\breve\sigma$ & 0.005 (0.00125 on some searches for the simulated data)\\
$\spatReg$ & 0.1 \\
$a$ & 0.5 \\
\hline
\end{tabular}
\caption{Algorithmic parameters used for applying IBPF to the measles model. The same values were used for each of the sub-models {\modelA}, {\modelB} and {\modelC}.}\label{tab:algpars}
\end{center}
\end{table}

\noindent Notes on Table~\ref{tab:algpars}:
\begin{enumerate}
\item The full $D\times \Time$ matrix of perturbation standard deviations with entries $\sigma_{d,n}$ was reduced to a single algorithmic parameter, $\breve\sigma$, via
\begin{equation}
\nonumber
\sigma_{d,n} =
  \left\{ \begin{array}{ll}
    \breve\sigma & \mbox{if $\theta_d$ is not an IVP and not $\alpha_{\unit}$}
    \\
    0.1\, \breve\sigma & \mbox{if $\theta_d$ is $\alpha_{\unit}$}
    \\
    2\, \breve\sigma & \mbox{if $\theta_d$ is an IVP, and $n=0$}
    \\
    0 & \mbox{if $\theta_d$ is an IVP, and $n\ge 1$}
  \end{array}\right.
\end{equation}
where the initial value parameters (IVPs) are $\pinit_{S,u}$, $\pinit_{E,u}$ and $\pinit_{I,u}$.

\item Parameters perturbations were carried out on a transformed scale. A logit transform was used for $\pinit_{S,u}$, $\pinit_{E,u}$, $\pinit_{I,u}$, $\rho_{\unit}$. A log transformation was used for $\measOD_{\unit}$, $\sigma_{SE,u}$, $\mu_{EI,u}$, $\mu_{IR,\unit}$, $\meanBeta_{\unit}$, $\alpha_{\unit}$, $\gravity_{\unit}$, $\iota_{\unit}$. No transformation was used for $\amplitude_{\unit}$.

\item The algorithmic parameters do not have any scientific significance once successful maximization has been demonstrated. They may affect the ease of successful maximization, or even the ability to attain this within an acceptable level of Monte Carlo uncertainty.

\item $M=100$ was chosen empirically.
For a computationally challenging maximization problem, we expect to carry out many searches from a variety of starting values, and we conduct further experiments following up on successful leads.
In this case, it is enough to choose $M$ so that each search has a fair chance of finding a higher likelihood when there are local improvements to be made.

\item Numerical experiments were carried out to choose $J$.
We look for the smallest $J$ such that there is not much to be gained by making $J$ larger.
This is carried out by looking for evidence about the bias and variance (discussed further in Section~\supSecEfficiency) which both need consideration when carrying out likelihood ratio tests and Monte Carlo adjusted profile confidence intervals \citep{ionides17,ning21}.

\item Each iteration of IBPF produces a log-likelihood estimate corresponding to the extended model with dynamically perturbed parameters. At the end of the search, the log-likelihood was re-evaluated using BFP, with 10 replications at $J=8000$ particles. A high level of effort on likelihood evaluation assists the task of building understanding about the likelihood surface from repeated Monte Carlo searches.

\item The exact record of all our computations is the source code for our numerical results, which is available at \url{https://github.com/ionides/ibpf_article}.

\end{enumerate}

%\newpage

\section{Computational efficiency}

Numerical error of Monte Carlo methods can be decomposed as bias and variance, and efficiency corresponds to how error scales with the amount of computational resources used. Expended resources can be quantified by objective metrics such as joules or dollars, but in practice we usually assess resources in terms of computational time on the machines that we personally have available.

Here, our main goals are evaluation and maximization of the log-likelihood.
Variance in Monte Carlo likelihood estimates results in negative bias on the log-likelihood due to Jensen's inequality.
Approximations involved in constructing a filter provide another source of bias for estimating the log-likelihood.
This Monte Carlo approximation bias has negative expectation, over when the model is correct, since log-likelihood is a proper scoring rule \citep{gneiting07}.
Monte Carlo maximization error (meaning the difference between the unknown, exact log-likelihood at the Monte Carlo MLE compared to the unknown, exact MLE) can only be negative.
Based on these considerations, we seek methods giving high average Monte Carlo log-likelihood at a Monte Carlo MLE, and our practical goal is to obtain reliably high log-likelihoods using calculations taking no longer than a day or so on one 36-core computing node.

%\newpage


\section{Varying the spatial autoregression parameter, $\spatReg$}

\fontsize{9}{11.5pt plus.8pt minus .6pt}\selectfont

<<v-v6_start,echo=F>>=
readRDS("v/v6_3/initial_lik.rds") -> loglik_start_spatreg
median_start_spatreg <- median(loglik_start_spatreg[,"start_loglik"])
@

<<spatreg_boxplot, echo=F, fig.height=3.6, fig.width=5.5, out.width="4.5in", fig.pos="h", fig.cap = paste0("Varying the IBPF spatial autoregression algorithmic parameter for shared parameters with simulated measles data. The log-likelihood was obtained using $M=100$ iterations of IBPF starting at random parameters (median starting log-likelihood, ",signif(median_start_spatreg,3), "). The horizontal dashed line denotes the log-likelihood at the true parameters.")>>=
library(foreach)
loglik_truth <- -38882.7 # from w1.r
spat_regression <- c(0, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1) 
spat_regression_char <- c("0", "1/64", "1/32", "1/16", "1/8", "1/4", "1/2", "1")
Experiment <- length(spat_regression)
results <- foreach(experiment=1:Experiment,.combine=rbind) %do% {
  r <- readRDS(file=paste0("v/v6_3/global_e",experiment,".rds"))
  cbind(r,sp_reg=spat_regression[experiment])
}
loglik_max <- max(c(loglik_truth,results$loglik))
results <- as.data.frame(results)
par(mai=c(0.6,0.8,0.1,0.1))
par(family="serif")
boxplot(loglik~sp_reg,data=results,ylim=c(-40000,loglik_max),
  ylab="Log-likelihood", xlab="spatial autoregression parameter",
  names=spat_regression_char)
abline(h=loglik_truth,col="blue",lty="dotted")
@

}



\end{document}
